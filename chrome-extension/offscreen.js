
/* global chrome */

let mediaRecorder = null;
let audioChunks = [];
let isRecording = false;

console.log('ðŸ”µ Offscreen document loaded');

// Handle messages from background script
chrome.runtime.onMessage.addListener((message, sender, sendResponse) => {
  console.log('ðŸ“¨ Offscreen received message:', message.type);
  
  if (message.type === 'ping') {
    console.log('ðŸ“ Responding to ping');
    sendResponse({ success: true, message: 'pong' });
    return;
  }
  
  if (message.type === 'start-transcription') {
    console.log('ðŸŽ¬ Starting transcription with stream ID:', message.streamId);
    startAudioCapture(message.streamId)
      .then(() => {
        console.log('âœ… Audio capture started successfully');
        sendResponse({ success: true });
      })
      .catch(error => {
        console.error('âŒ Failed to start audio capture:', error);
        sendResponse({ 
          success: false, 
          error: error.message,
          errorType: error.name
        });
      });
    return true; // Keep message channel open for async response
  }
  
  if (message.type === 'stop-transcription') {
    console.log('ðŸ›‘ Stopping transcription');
    stopAudioCapture()
      .then(() => {
        console.log('âœ… Audio capture stopped');
        sendResponse({ success: true });
      })
      .catch(error => {
        console.error('âŒ Error stopping audio capture:', error);
        sendResponse({ success: false, error: error.message });
      });
    return true;
  }
  
  console.warn('â“ Unknown message type:', message.type);
  sendResponse({ success: false, error: 'Unknown message type' });
});

async function startAudioCapture(streamId) {
  if (isRecording) {
    console.log('Already recording, stopping first...');
    await stopAudioCapture();
  }
  
  try {
    console.log('ðŸ“¡ Getting user media with stream ID:', streamId);
    
    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        mandatory: {
          chromeMediaSource: 'tab',
          chromeMediaSourceId: streamId
        }
      },
      video: false
    });
    
    console.log('âœ… Got media stream, tracks:', stream.getAudioTracks().length);
    
    if (stream.getAudioTracks().length === 0) {
      throw new Error('No audio tracks found in stream');
    }
    
    // Find best supported MIME type
    const mimeTypes = [
      'audio/webm;codecs=opus',
      'audio/webm',
      'audio/mp4',
      'audio/ogg;codecs=opus'
    ];
    
    let selectedMimeType = '';
    for (const mimeType of mimeTypes) {
      if (MediaRecorder.isTypeSupported(mimeType)) {
        selectedMimeType = mimeType;
        break;
      }
    }
    
    if (!selectedMimeType) {
      throw new Error('No supported audio MIME types found');
    }
    
    console.log('ðŸŽ­ Using MIME type:', selectedMimeType);
    
    mediaRecorder = new MediaRecorder(stream, {
      mimeType: selectedMimeType,
      audioBitsPerSecond: 64000
    });
    
    audioChunks = [];
    
    mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        console.log('ðŸ“¦ Audio chunk received:', event.data.size, 'bytes');
        audioChunks.push(event.data);
        
        // Process chunks when we have enough
        if (audioChunks.length >= 3) {
          processAudioChunks();
        }
      }
    };
    
    mediaRecorder.onerror = (event) => {
      console.error('ðŸ”´ MediaRecorder error:', event.error);
    };
    
    mediaRecorder.onstop = () => {
      console.log('â¹ï¸ MediaRecorder stopped');
      if (audioChunks.length > 0) {
        processAudioChunks();
      }
    };
    
    mediaRecorder.start(1000); // 1 second intervals
    isRecording = true;
    
    console.log('ðŸŽ¬ MediaRecorder started');
    
  } catch (error) {
    console.error('ðŸ’¥ Error in startAudioCapture:', error);
    throw error;
  }
}

function processAudioChunks() {
  if (audioChunks.length === 0) return;
  
  try {
    const audioBlob = new Blob(audioChunks, { 
      type: mediaRecorder ? mediaRecorder.mimeType : 'audio/webm' 
    });
    
    console.log('ðŸŽµ Processing audio blob, size:', audioBlob.size, 'bytes');
    
    // Clear chunks
    audioChunks = [];
    
    // Simulate transcription for now
    simulateTranscription();
    
  } catch (error) {
    console.error('ðŸ’¥ Error processing audio chunks:', error);
  }
}

function simulateTranscription() {
  const phrases = [
    "What are your greatest strengths?",
    "Tell me about yourself",
    "Why do you want to work here?",
    "Describe a challenging project",
    "Where do you see yourself in 5 years?"
  ];
  
  const randomPhrase = phrases[Math.floor(Math.random() * phrases.length)];
  
  setTimeout(() => {
    console.log('ðŸŽ¤ Sending simulated transcription:', randomPhrase);
    
    try {
      chrome.runtime.sendMessage({
        type: 'transcription-result',
        text: randomPhrase,
        timestamp: Date.now()
      });
    } catch (error) {
      console.error('ðŸ’¥ Error sending transcription:', error);
    }
  }, 500 + Math.random() * 1000);
}

async function stopAudioCapture() {
  console.log('ðŸ›‘ Stopping audio capture...');
  
  isRecording = false;
  
  if (mediaRecorder && mediaRecorder.state !== 'inactive') {
    try {
      mediaRecorder.stop();
      
      // Get all tracks and stop them
      if (mediaRecorder.stream) {
        mediaRecorder.stream.getTracks().forEach(track => {
          track.stop();
        });
      }
    } catch (error) {
      console.warn('âš ï¸ Error stopping MediaRecorder:', error);
    }
  }
  
  mediaRecorder = null;
  audioChunks = [];
  
  console.log('âœ… Audio capture stopped completely');
}

// Cleanup on unload
window.addEventListener('beforeunload', () => {
  console.log('ðŸ”„ Offscreen unloading, cleaning up...');
  stopAudioCapture();
});

console.log('âœ… Offscreen script ready');
